## CHAPTER 4

## Herds and Headaches

## How Our Bodies Are Programmed to Care About Popularity

An old house sits at the top of a wooded hill, a cape of fog swirling around it in the darkness. The only light comes from a single window shrouded with overgrown vines and a broken shutter. Inside lies a man in a tangle of sheets on a bed with a headboard as tall and heavy as a tombstone. He is frail, wheezing, yet desperately clutching something made of glass. His dry, crusted lips part; the sound of air slowly escapes his body.

“Ro . . .” he moans. “Rose . . .”

His eyes open suddenly, staring straight ahead with pain, fear, then anger. He finally finds the strength to utter:

“Rosenbaum!”

From his hand drops a frame holding a picture of his childhood bully, Damien Rosenbaum. It falls to the floor, the glass shattering to pieces as the last sound of air emerges from the limp body above . . .

OK, none of this actually happened. But it could have. Recent evidence suggests that being unpopular can be hazardous to our health. In fact, it might even kill us.

That’s because our bodies are literally programmed to make us care about popularity. More on that later . . .

—

First, a quick quiz:

Do you own an iPhone/iPad?

Do you drive a Honda?

Have you recently bought something from Amazon?

Do you use a Gillette razor?

Have you purchased anything made by Disney recently?

Did you drink a Coca-Cola product this week?

Do you have a Gmail address?

If you answered yes to at least one of these questions, then you’re pretty much like most of the rest of the world. It is part of human nature to follow trends. The above list was adopted from a report on the largest global brands, and while some of their success reflects the fact that they offer excellent products and services, it also has to do with popularity. We like to follow the herd, and we tend to rely on one another substantially when we make our own choices. There is something about popularity that is inherently appealing.

Princeton sociologist Matthew Salganik and his colleague at Yahoo!, Duncan Watts, conducted a series of experiments to test just how strongly drawn we are to popularity. In one study, over twelve thousand participants from North America, South America, and Europe were asked to visit an internet site to rate some new music and download it for free. The site wasn’t real but was developed by the experimenters to make it look similar to iTunes. Through this study, Salganik and Watts were able to examine how our taste in music relies on that of others.

Participants were initially presented with a list of forty-eight unfamiliar rock songs. They were instructed to listen to each, rate it from one to five stars, and then, if they wished, download the song for free. In this first phase of the study, the experimenters simply collected data on the popularity of each song. For instance, “She Said” by the group Parker Theory was by far the most popular—15 percent of all participants who heard it chose to download it. “Florence” by the group Post Break Tragedy was the least popular, with only 1 percent of listeners downloading it. Much like on iTunes, the researchers used these data to create a list of the songs organized by their popularity. “She Said” was listed first, with the most downloads, and “Florence” was last.

Then, the experimenters began their study on conformity by testing what might happen if they manipulated the ranking of each song. For their next group of participants, they simply inverted the data on each song’s download rate: “Florence” was now ranked as the most downloaded choice, while “She Said” was the least. With this manipulated ranking posted, the experimenters opened the music portal again and watched to see how each song’s apparent popularity affected the participants’ musical choices over time. As you may expect, popularity mattered—the download rate of “Florence” increased tenfold, while that of “She Said” plunged to only 2 percent. And as more people downloaded “Florence,” it was made to seem even more popular, leading in turn to even more downloads, while “She Said” remained at the bottom of the pack.

Results like these help explain why popularity itself has become such a valued business commodity. Marketers know that we tend to follow the herd, so they rely on who or what is popular to influence our behaviors.

Have you read an article online recently? You may have noticed that the headline is no longer the only thing placed at the top of the page meant to attract our attention. There’s often also a row of icons above each article, one for each social media outlet, with a running tally showing how often the piece has been emailed, “liked” on Facebook, tweeted, and so on. Lists of “trending topics” have become common, too—not just on social media, but on news outlets as well. This information is intended to pique our interest, as if the popularity of the story among others should make it more enticing to us. The tactic is not much different from a TV commercial that touts the “number one movie in America” or the “leading brand of headache medicine.” In each case, we are called to follow the herd.

We are in turn prompted to tell others what we liked, or bought, or preferred, so that the herd can follow us as well. As we finish reading an article that impresses us, we are prodded to “like” it or email it to our friends. Likewise, when we buy products, we are asked to post the news to our Facebook feed. I can’t imagine that my friends would be interested in knowing that I just bought shaving cream, but I can understand why the manufacturer wants me to tell them I have. We associate popularity with quality.

Why is this strategy so effective when, logically, it doesn’t make much sense? Why should I care about what everyone else is reading? I want to read what interests me, not what appeals to ten thousand complete strangers. I want to watch movies that match my own tastes, and I assume that my own body’s physiology is the most important factor to consider when choosing a medicine.

One explanation is based on the idea that we feel essentially similar to others, thus, we assume that whatever the herd likes, we’d like, too. Or perhaps our natural proclivities toward popularity come from our sense of community and our desire to feel connected. If everyone is talking about a news item, or a movie, we don’t want to be left out of the conversation.

It’s interesting that despite all reason, we remain naturally tuned in to popularity. Yet this instinct doesn’t always help us. Sometimes our tendency to follow the herd can have serious consequences.

Economists suggest that the lure of popularity has been responsible for some of the most peculiar and damaging trends in history. In 1841, Scottish journalist Charles MacKay wrote about the human impulse to follow the herd in his famous book _Memoirs of Extraordinary Popular Delusions and the Madness of Crowds_ , in which he examined the tendency for an asset to gain in value well beyond its intrinsic worth simply because it is perceived as popular—a phenomenon we now refer to as a “market bubble.” In one chapter, he recounts the great fervor that ignited in the early seventeenth century over a particular tulip species that had been imported to Holland. This blossom offered no apparent superior value in its beauty, scent, or longevity when compared to indigenous species, but its popularity grew nonetheless. As this passion for the tulip spread from the aristocrats to the middle class, and eventually to those with scant means, the flower’s value soared, ultimately garnering huge sums for even a blossom weighing less than a gram. Reportedly, this type of tulip became so valuable that visitors to Holland were imprisoned if they unwittingly damaged a bulb. Such “overvaluation” of a commodity based on its popularity rather than on its actual worth is the basis for an unsustainable market, and is the same phenomenon that accounted for the stock market’s dot-com bubble of the 1990s.

As more Dutch were afflicted by what MacKay called “Tulipomania” and the popularity of the flowers increased, prices continued to rise. The Dutch assumed that citizens from throughout Europe would share their enthusiasm, bringing increased value to their investments. Tulip dealers accordingly mortgaged their homes and spent their fortunes to purchase more bulbs, while businessmen began neglecting other profitable industries. Of course, the tulips never turned out to be worth anything near what people had paid for them, and when the flower’s price dropped dramatically, the collapse of the market bubble ultimately threatened the entire Dutch economy. MacKay concluded, “Men . . . think in herds; it will be seen that they go mad in herds, while they only recover their senses slowly, and one by one.”

In my own research, I have found that the instinct to embrace the popular can lead to behaviors with even worse consequences. My work has been designed to understand how herd-following patterns begin when we are young, and specifically just how far youth will go to be like one another. What will adolescents do when they are told that their popular peers endorse behaviors that are dangerous or illegal, and how will they react when they are asked to be mean toward one another, even though they know it is morally wrong to do so?

In one study, my former colleague and now Stanford psychologist Geoff Cohen and I examined a range of risky behaviors. We asked kids about drinking alcohol, having sex without a condom, smoking marijuana, and using harder drugs, like heroin and cocaine. We also questioned them about bullying, dangerous eating behaviors like bingeing and purging, and using hormones and drugs to change their body shape.

In the United States, about one of every four adolescents has used alcohol before the age of fourteen, 25 percent have had five drinks in a row before high school graduation, one in five smoked pot before age fourteen, and 40 percent of teens report that they did not use a condom the last time they had sex. More than one out of ten say they have fasted just to try to look thinner. These are all remarkably risky behaviors that strongly predict which teens will become pregnant before graduating high school, grow up to have substance abuse problems, develop serious eating disorders, and even get cervical cancer.

Of course, when we asked adolescents in our study whether they would be likely to engage in these behaviors, the vast majority told us that drugs are bad and that everyone should use a condom, be nice to others, and keep a healthy attitude about physical appearance.

We then invited each of these same subjects to participate in a simulated online chat room along with three of their most popular fellow students. In fact, it was not a real chat room at all, but a computer program we developed using intricate graphics and timing to convince adolescents that they were talking live with the cool kids from their own school. The others in the chat room were phantoms, or “electronic confederates,” that we identified as highly popular by listing the first name and last initial of actual grade-mates in our participants’ school. Our deception worked. At the end of the experiment, when we revealed our procedures, our subjects told us they really believed they were online, and even reported excitedly that they believed they knew exactly which of their peers was taking part in their chat.

In this counterfeit chat room, we again asked adolescents the same questions about risky behaviors, but this time, we had the fictitious peers take part first and report that they would be very likely to engage in each bad habit. Our participants were then asked again to respond to the same questions, first while they believed their peers were watching their responses, and then after they had ostensibly logged out of the chat room, so we could make sure they weren’t just showing off for the cool kids.

What we found was that simply knowing that their popular peers would be likely to drink alcohol, smoke pot, or have unprotected sex was sufficient to change adolescents’ answers—and dramatically so. Suddenly our participants were far more apt to say they would engage in all these behaviors than they had been when they began our study. Even when they had logged off and were told that none of their peers were watching, our subjects continued to state that they would pursue those risky actions.

We then took the study one step further. Rather than simply asking adolescents what they would do hypothetically, we gave our participants the chance to actually do something they shouldn’t do in real life. After responding to a few simple questions about hobbies and interests within the chat room, during which we manipulated the responses of one confederate to seem a bit deviant from the others, we offered our participants the option to vote one of their “peers” off the experiment. They were instructed that for someone to be kicked out, he would have to be voted off by all of the others in the chat room unanimously. They were also told that the evicted participant would lose out on the chance to meet the others, and would not receive the reward we offered for completing the task.

In each case, the subjects were asked to cast the deciding vote: the fate of their peer was in their hands, and they had the choice to be kind or to be mean. What they didn’t know was that, in this case, the individual they were voting off wasn’t a real person. They were also unaware that the other votes were fabricated.

Once the subjects saw that their popular peers had voted against one of their own, eight out of ten of our participants voted to evict as well.

Why are we so likely to follow the herd?

On a sunny day in the year 60,000 BC in what is now southern Europe, a lone female enters a crowded cave where others are sitting down to eat their latest kill. But when she attempts to take her place at the rock where her hominin friends usually eat, she is shunned. It is the time of the full moon, and the others at her rock have a rule that on these days, females must wear fur clothes. This particular woman is wearing a wrap made of animal skin.

“You can’t sit with us!” the women seem to grunt. Finally, with no place else to eat, she leaves the cave, walks a few dozen yards, and sits alone. Moments later, she is attacked by a wooly mammoth and is never heard from again.

OK, this didn’t happen, either. But research suggests that there may be something about unpopularity then that has a lot to do with the humans we have become today.

Back in 60,000 BC, we were not the only humanlike species on the planet. Anthropologists believe that in addition to the beings who had migrated out of Africa and closely resembled our own species today, there were Neanderthals in the north, Denisovans in Asia, and even a small humanlike species called _Homo floresiensis_ in Indonesia. Yet only we humans ultimately survived. We endured not because we were the strongest—in fact, the Neanderthals were a bit larger, with bigger teeth, and probably could have won any battle against relatively weak humans. It wasn’t because we had bigger brains, either.

There is one quality that is unique among humans and has been credited to be a fundamental factor in our evolutionary advantage. While some species became larger, or stronger, or able to withstand more severe temperatures, it was we humans who learned how to work together. Anthropology research reveals that, unlike other hominins, humans had the genes to form and comprehend complex vocal sounds. Language ability formed the basis for more sophisticated social interactions. Soon we became a species that could organize into groups and network with our peers in ways that were far superior to those of the others.

Living as a herd offered many survival advantages. By working as a community and sharing tools, we could hunt more effectively. By sharing the spoils of the hunt, we could eat food while it remained fresh and safe to consume. Joining together in groups enabled us to warn and protect one another when predators threatened. We soon evolved to become acutely sensitive to social cues, and through the process of natural selection, our species eventually favored only those who were attuned to the herd. Those individuals who remained solitary became extinct.

It’s been thousands of years since we needed one another to survive our daily lives. Rarely does someone today venture out to Starbucks alone only to be attacked by a woolly mammoth. But the vestigial effects of our evolution as social creatures are still visible in many subtle ways. Have you ever wondered why yawns are contagious and the menstrual cycles of women living together synchronize? Some hypothesize that even these phenomena reflect our genetic programming as a social species. The herd worked most effectively when everyone was able to move as a single unit and stop together for resting, mating, or childbirth. Today, we still have instincts that make us become tired simultaneously or fertile at the same times.

—

So what happens if we don’t follow the herd and choose to remain alone, isolated, unpopular?

Over the past several decades, scientists have demonstrated that being unpopular can actually be harmful. It’s not hard to imagine how solitude can lead to emotional difficulties. Those who are ostracized, alienated, bullied, or victimized are more likely to experience loneliness, low self-esteem, anxiety, and depression. But there is now also evidence that being unpopular may even have dire consequences for our physical health—to the point that it can kill us.

Julianne Holt-Lunstad, a psychologist at Brigham Young University, recently conducted a meta-analysis—a study of studies—combining the data from 148 prior investigations. Each asked the same basic question: does being unpopular increase the risk of death? Collectively, these studies included 308,000 participants between the ages of six and eighty from all over the world. Each included two basic procedures. First, the investigators measured the size of participants’ social networks, the number of their friends, whether they lived alone, and the extent to which they participated in social activities. Then, they followed each participant for months, years, and even decades to track their mortality rate.

The results revealed that being unpopular—isolated, disconnected, lonely—actually predicts mortality rates. But perhaps even more surprising is just how powerful these effects can be. People in the study who had larger networks of friends had a 50 percent increased chance of survival by the end of the study. It didn’t matter whether the participants were male or female, whether they had health problems to begin with, or where in the world they lived. Being disconnected from the herd substantially increased the risk of death.

But not every kind of connection was equally important. And this finding was key, because it gives us a clue as to which type of popularity really matters.

Simply living with someone, or having a spouse, was related to increased life expectancy, but not very significantly. It was those people who actively participated in their social lives and had _good-quality_ relationships who seemed to benefit the most. In other words, it was those individuals who had relationships that the _most likable_ people tend to have that seemed to have the advantage. Their chance of survival was 91 percent higher than those who were essentially alone. In other words, almost twice the number of popular people were alive at the end point of the study as those who were unpopular. This is a highly significant finding. Comparing these figures to research on established health risks suggests that being unpopular more strongly increases our chance of death than does obesity, physical inactivity, or binge drinking. In fact, the only factor comparable to unpopularity as a health hazard is smoking!

How could our social lives, or lack thereof, kill us? Could effects like these be accounted for by intentional self-harm? Perhaps those most socially isolated, ostracized, or friendless are especially likely to commit suicide?

This is certainly true. In the United States, suicide is the second leading cause of death in adolescence and young adulthood; it remains one of the top ten causes of death until the age of sixty-five. One of the most common risk factors for suicide attempts is feeling lonely, like a burden to others, or like one doesn’t belong. Among adolescents in particular, ostracism from a peer group is an especially strong predictor of suicidal behavior. We are painfully reminded of this every time we hear of another teen who commits suicide after being tormented in school or online.

But remarkably, intentional self-harm does not account for the link between unpopularity and mortality. In Holt-Lunstad’s meta-analysis, studies that measured death by suicide were excluded.

In fact, recent evidence suggests that those who are socially disconnected are at risk for a wide range of physical health problems that can cause death. In 2016, Kathleen Mullan Harris, a sociologist at the University of North Carolina at Chapel Hill, examined how social connections might predict coronary artery disease, hypertension, cancer, and stroke. Her research accessed data from four large, nationally representative samples that collectively included about fifteen thousand Americans between the ages of twelve and eighty-five. As with Holt-Lunstad’s meta-analysis, Harris’s group examined first social integration and then a range of physical health indices between five and twelve years later.

What Harris found was that having friends or a romantic partner, socializing with neighbors, and volunteering substantially decreased the risk of physical illness. Those who were socially isolated when the study began were most likely to develop high blood pressure. They were also most likely to have high levels of C-reactive protein in their blood—a harbinger of inflammation-related health problems, like rheumatoid arthritis, inflammatory bowel disease, and heart attacks. None of these findings seemed to have any relation to participants’ gender, race, educational attainment, income, history of smoking, alcohol use, physical activity, stress, or depression. Of course, it is impossible to ascertain whether it was unpopularity that _caused_ these health problems per se, but the results are among the most powerful to suggest that even after accounting for so many other possible explanations, social isolation seemed to be the most powerful prognosticator of illness years later.

We now know at least a few reasons why being popular, even as adults, is more important than we ever imagined. One simply has to do with the psychological effects of unpopularity. Being disliked means that we lack social support, so in times of stress, we have no one to turn to, to help us out of trouble. In one study of women with breast cancer, simply participating in a support group with other patients was a significant predictor of life expectancy, even after accounting for other possible factors that could have explained these effects.

The effects of popularity can even be seen in our body’s physiology. Recent evidence suggests that our connection to fellow humans has a strong effect on cortisol—a hormone that is produced as part of the autonomic nervous system in response to stress. Cortisol can be beneficial—in the face of a looming attack, it maintains the fight-or-flight response. When this hormone is released, our hearts pump more blood to our muscles, our airways expand so our brains can get more oxygen, and our body fat releases blood sugar so we can maintain our energy while we bolt or battle.

But cortisol has a Goldilocks-like quality: too much or too little can wreak havoc on a wide range of bodily functions. High levels of cortisol weaken our immune systems, leading to obesity, heart disease, gastrointestinal disorders, and even infertility. They can also damage the cortisol response system itself until it is no longer responsive to stress, much like overused shock absorbers compromise the performance of a car. Too little cortisol puts us at risk for chronic fatigue, asthma, rheumatoid arthritis, eczema, and so on.

Can popularity affect cortisol levels, and thus place unpopular people at greater risk for dire health consequences?

My graduate student Casey Calhoun and I set out to determine whether that was the case. We invited about two hundred adolescent girls into our lab and measured a wide range of their prior social experiences. Then, we exposed them to a minor stressor and measured the cortisol in their saliva to assess whether the social experiences of these girls would influence how their bodies responded to stress. We did so by asking each girl to face a camera with a feedback screen and deliver an impromptu speech. A young male onlooker sat directly in front of them, ostensibly judging their performance. Giving a speech is a very safe task commonly used by social scientists to induce stress and provoke cortisol output.

As we expected, many of our participants had mild elevations in cortisol while giving their speech but quickly recovered to normal levels within fifteen to twenty minutes, a normal span of time. But some subjects as young as twelve already had abnormal responses. Their bodies produced too little cortisol, indicating an under-reactive stress response system. This was a sign that their brains were not prepared to handle stress, which put them at risk for developing later health problems.

Why did these girls have inadequate cortisol responses? One of the strongest predictors of this reaction was the extent to which they had been teased, excluded by others, and called names by peers in the past. It didn’t matter how old they were, or how many other stressful things they had experienced in their lives. The results also were not accounted for by their race, ethnicity, or symptoms of depression. The more unpopular they were, the more their cortisol response systems were severely compromised.

If unpopularity prevents our bodies from responding adequately to stress, then do social connection and support help us react more adaptively? We examined this question in a second study also involving adolescent girls and a speech task. But in this experiment, we asked girls to bring their best friends along.

After the subjects delivered their talks before the video camera, the girls, naturally, discussed them. Many of their friends were very supportive of their effort, listening carefully, helping the girls feel better about themselves after the speech, and being generally empathic. But the friends of some other girls were less responsive, and in fact, a few were so focused on themselves that they barely engaged in mutual conversation at all.

In this experiment, we focused on those girls who had an over-reactive cortisol response to stress—also an indicator of future health difficulties. For this group, cortisol levels spiked after the speech and remained high for far longer than they did for other girls in our study. But we found that the more girls’ friends were supportive after the talks, the more quickly cortisol levels reverted to normal. Overall, our results demonstrated that social experiences have remarkable power over our stress response system.

More recently, psychologists and neuroscientists have learned that the link between our membership in the herd and our health may go even deeper. It is not only when we feel stressed that being socially connected matters. Being unpopular may be sufficient to harm us on its own.

Mary Sue’s life changed forever late one afternoon. It was a nice day, and in fact, all days were nice for Mary Sue. She walked to school, sat with her friends at lunch, and then came home to milk and cookies that her mother had left out for her. Everything about Mary Sue’s life seemed perfectly pleasant. But there was something out of the ordinary. Mary Sue and everyone in her entire town existed in shades of black and white: their hair, their eyes, their clothes, their skin—everything.

When evening came, Mary Sue did her homework at a desk in the corner of her room, a cardigan draped over her shoulders and a pair of horn-rimmed glasses hanging from a chain around her neck. It started to get chilly, and she crossed the room to close the window. Down on the street she saw a boy she liked standing and looking up at her. He reached down, lifted a rock, and then sneered as he threw it at Mary Sue’s window. The rock landed in the middle of her bedroom floor, surrounded by shattered glass.

As he drove off with a group of laughing friends, Mary Sue stood in shock. How could something so horrible happen? Why would she be the target of a deed so . . . well, so unpleasant? Then she felt something she had never felt before: her heart ached; tears ran down her face. Suddenly, Mary Sue’s hair turned blond, her eyes became blue, and her lips pink. That act of unkindness had not only broken Mary Sue’s heart but seemed to change every cell in her body. Neither Mary Sue nor her town would ever be the same . . .

OK, this scenario also is adapted from a movie. But is an experience such as Mary Sue’s strictly fiction? How does rejection actually get under our skin and transform us?

Have you ever noticed that when people talk about feeling lonely, rejected, or unpopular, they tend to use words typically associated with physical illness? Terms like “heartbreak” or “homesick,” emotional “scars,” and “hurt” feelings are common to many languages. Are these just expressions, or is there something about unpopularity that can actually do us physical harm?

UCLA neuroscientist Naomi Eisenberger wondered the same thing and took a special interest in the question of whether unpopularity may affect us in more fundamental ways than we are aware of. She conducted a series of studies designed to examine the regions of our brain that become activated when we experience social rejection. She did so by having her participants take part in a computer game designed to simulate a negative interaction with peers.

Imagine playing catch over the computer with two other players who, you are told, are in nearby rooms. On-screen are two stick figures, one on each side, representing the other players, and a hand in the center representing you. To take part in the game you hold a joystick, and when someone throws a ball to you, you catch it, and then choose to whom you would like to throw it next by moving the joystick left or right.

This game is called Cyberball, and it was developed by researchers to understand social experiences. In Eisenberger’s study, subjects were scanned in an fMRI machine while playing. But unbeknownst to them, there were no other actual participants in the game—the stick figures were controlled by a computer simulation program. For about ten minutes, the program ensured that the ball was thrown to each player an equal number of times.

But then, without explanation, the program made it appear as though the other two players had decided to exclude the study participant from the game. Imagine watching as the ball is tossed back and forth, over and over, but never to you. Eisenberger let the game go on in this manner for another ten minutes.

During this latter period the researchers noticed something interesting happening: according to fMRI results, the parts of the brain that were activated during this part of the experiment were the same as those that are involved when we experience physical pain. Two regions in particular surprised Eisenberger—the dorsal anterior cingulate cortex (dACC) and the anterior insula (AI). Of course, the participants playing Cyberball did not experience any actual pain. The part of our brains responsible for the sensation of burning, stinging, or aching is elsewhere. But it is the dACC and the AI that work with our sensory input to interpret those sensations and tell us if we’re feeling something extremely unpleasant. In fact, these regions are part of the most powerful alarm systems in the brain, motivating us to escape the source of our pain at all costs. In short, Eisenberger found that at least some regions of our brain experience unpopularity in the same way that they respond to physical distress—a phenomenon that she refers to as “social pain.”

Subsequent research found that these same regions are activated during a whole host of social rejection experiences. As soon as we fear that we might get rejected from the group, our brain sends the most powerful signal at its disposal to warn us and motivate us back into the fold. Worrying about a breakup, seeing pictures of someone being teased, remembering a lost friend or loved one, or even just thinking about being negatively judged by others in the future all seem to implicate the same brain regions.

The neural overlap between social pain and physical pain has been identified in several other studies as well. For instance, research has found that those who have a low tolerance for physical pain also seem to be more sensitive to interpersonal rejection, and vice versa. In one experiment, Eisenberger even found that taking a Tylenol can actually reduce the sensation of social pain. Our brains try to ease the pain of headaches and heartbreaks in the exact same way.

Unpopularity also is felt in millions of other places in our bodies simultaneously and just as quickly: within our cells. Every day we lose millions of cells as they die off, and new ones are born, built to specifications dictated by our DNA. But there’s an interesting thing about DNA: it contains far more information than is needed for any given cell. Some of its genes are turned on, while others are left off, depending on where in our bodies the cell is located. It’s kind of like when you buy a computer that has a lot of software preloaded—some has already been activated to help make the computer run, but other software just sits on the desktop, dormant, waiting for you to double-click it.

So, if a cell is located in a kidney, the parts of DNA with the blueprint for a kidney cell are activated, while the gene that determines, say, the color of eyes remains inactive. This is useful—we don’t want an eye growing out of our kidney—so that part of the DNA strand literally coils up and moves to the edge of the nucleus, far from the center where DNA gets double-clicked.

Recently, neuroscientists discovered that unpopularity affects that mechanism. At the first sign that we may be banished from the group, our DNA unravels and reorients. In fact, social rejection experiences activate a surprisingly large number of genes, while also deactivating many others.

UCLA psychologists George Slavich and Steve Cole, experts in the field of human social genomics, have described DNA as being “exquisitely sensitive to social rejection.” They studied what happens immediately after we’ve been dumped by a romantic partner, excluded from a social event, rejected by a stranger, or even simply told that we may be socially evaluated by others we care about. Within forty minutes, they found, a wide array of changes in our DNA can be detected in the blood. Only a few dozen out of at least twenty thousand genes turn on or off in these moments, but even that small number seem to play a very significant role.

According to Slavich and Cole, these activated genes have a radical effect on the immune system. Some are linked to the body’s inflammation response, which comes in handy when we need to heal wounds or fight off bacterial infections. Slavich and Cole suggest that this response to rejection may be nature’s mechanism to help those who were unpopular. Millennia ago individuals who had no peers to protect them faced a high risk of an untimely death due to injury or attack. Those whose bodies preemptively activated a “pro-inflammatory” response that would be ready to heal them from any impending wounds were the most likely to survive. Ultimately, evolution favored bodies that were quickest to respond, and thus most sensitive to rejection.

Other genes implicated in this process are related to viral protection; social rejection seems to deactivate these DNA. Slavich and Cole suggest that those who had no peers to protect them no longer had a great need to be protected from viruses—who would infect them?—so their bodies conserved energy by reducing their vigilance to infection.

But today our lives are different. There’s no longer a great need for our immune systems to respond to the dangers associated with loneliness. Being unfriended on Facebook doesn’t require a systemic inflammation response. Our bodies, however, continue to respond as they did sixty thousand years ago. Today humans suffer from a wide range of diseases related to chronic inflammation, like cancer, asthma, Alzheimer’s, Crohn’s, hepatitis, lupus—the list goes on and on. We’re also very likely to catch a cold.

Our DNA doesn’t reorient itself only when we actually experience severe social rejection. Such changes occur even at the most subtle suggestion that we may be shunned. There’s even research to suggest that our pro-inflammatory genes are activated when we merely imagine being rejected, or when we play a video game that simulates our being left out.

Why, then, don’t we all fall ill after every heartbreak and betrayal? It’s likely that we _do_ experience an inflammatory response on such occasions, but only in a few cells out of the thirty-seven trillion in our bodies. It’s those who are chronically rejected who may suffer harm from these hypersensitive cells. Slavich suggests that unpopularity, even if it occurs over a period of just a few months, may be sufficient to trigger an entire “molecular remodeling” of the body as cells are gradually replaced by those containing DNA that’s hypersensitive to social rejection.

Is this a cause for concern?

Holt-Lunstad, the BYU psychologist who conducted the meta-analysis on popularity and mortality, believes so. She argues that despite all of our attempts to create ways to feel more connected than ever, we have never been more apart. Today, we are more likely to live alone, get married later in life, and move our families farther from our loved ones than ever before. In just the past twenty years, the number of people reporting that they feel they have no close confidant has tripled.

Our species is programmed to care about popularity. But we may be searching for connections in all the wrong places. What does this mean for our future?

Thomas was walking through a city surrounded by others, but still he felt alone. He could see those around him and even talk to them, but none of it seemed real. Soon, he realized that he was not really connected to other people at all. He was actually strapped to a computer, linked to others at their computers, all networked within a matrix of simulated interactions. Everything was mediated by technology, although everyone secretly longed for genuine social interaction. They built ever more complex programs to help them network across the globe, rapidly share information, and simulate real human discourse. But it didn’t work. It just made the people feel farther apart. Thomas and a small group of others discovered the truth—that their lives had been taken over by the machines—and dedicated themselves to bringing the world back together again, so no one would be alone, and no one would be unpopular.

This sounds like a movie, too, right? But it is a true story.
